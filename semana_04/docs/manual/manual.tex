\documentclass[11pt, a4paper]{article}

% --- Paquetes Fundamentales ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathpazo}       % Fuente elegante
\usepackage[spanish, es-tabla, es-noquoting]{babel} % <--- FIX: es-noquoting evita el conflicto con >
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage[most]{tcolorbox}

% --- FIX CR√çTICO PARA TIKZ + ESPA√ëOL ---
\usetikzlibrary{babel} % <--- Importante: Permite usar flechas -> sin errores
\usetikzlibrary{arrows.meta, positioning, calc, shapes}
\pgfplotsset{compat=1.17}

% --- Geometr√≠a ---
\geometry{top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}

% --- Colores Corporativos ---
\definecolor{primary}{RGB}{0, 100, 0}
\definecolor{secondary}{RGB}{34, 139, 34}
\definecolor{accent}{RGB}{255, 140, 0}
\definecolor{danger}{RGB}{220, 20, 60}
\definecolor{codebg}{RGB}{245, 247, 250}

% --- Configuraci√≥n de Encabezados ---
\pagestyle{fancy}
\fancyhf{}
\lhead{\small \textbf{AgroFuture AI Training}}
\rhead{\small \textit{Semana 04: Modelado Predictivo}}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% --- Cajas Personalizadas ---
\newtcolorbox{agrobox}[1]{
  colback=secondary!5!white, colframe=secondary,
  title={\textbf{#1}},
  fonttitle=\bfseries, boxrule=0.5mm, arc=2mm, shadow={2mm}{-2mm}{0mm}{black!10}
}

\newtcolorbox{mathconcept}[1]{
  colback=blue!5!white, colframe=primary,
  title={$\boldsymbol{\Sigma}$ #1},
  fonttitle=\bfseries, boxrule=0.5mm, arc=2mm, sidebyside align=top
}

\newtcolorbox{warningbox}[1]{
  colback=red!5!white, colframe=danger,
  title={‚ö†Ô∏è #1},
  fonttitle=\bfseries, boxrule=0.5mm, arc=2mm
}

% --- Estilo de C√≥digo ---
\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{codebg},
    commentstyle=\color{gray}\itshape,
    keywordstyle=\color{blue}\bfseries,
    numberstyle=\tiny\color{gray},
    stringstyle=\color{secondary},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=l,
    rulecolor=\color{secondary},
    numbers=left,
    captionpos=b,
    showstringspaces=false
}
\lstset{style=pythonstyle}

% --- Metadatos ---
\title{\Huge \textbf{Modelado Predictivo en Agroindustria} \\ \Large De la Qu√≠mica del Suelo a la Predicci√≥n de Cosechas}
\author{\textbf{M√≥dulo de Ingenier√≠a de Datos e IA}}
\date{\today}

\begin{document}

\maketitle
\thispagestyle{empty}

\begin{abstract}
    \noindent \textit{Este documento t√©cnico introduce los fundamentos matem√°ticos y pr√°cticos de la Regresi√≥n Lineal aplicada al contexto agr√≠cola. Aprenderemos a traducir datos crudos de sensores y an√°lisis de suelos en modelos matem√°ticos capaces de estimar rendimientos futuros.}
\end{abstract}

\tableofcontents
\newpage

% -----------------------------------------------------------------------------
\section{Introducci√≥n: El Valor del Dato}
% -----------------------------------------------------------------------------

En la agroindustria moderna, el dato es el nuevo fertilizante. Sin embargo, tener datos almacenados en una base de datos no genera valor por s√≠ mismo. El valor surge cuando usamos esos datos para responder preguntas de negocio:

\begin{itemize}
    \item \textbf{Descriptivo:} \textquestiondown Cu√°nto cosechamos el a√±o pasado? (SQL/Pandas)
    \item \textbf{Predictivo:} \textquestiondown Cu√°nto cosecharemos el pr√≥ximo mes si reducimos el nitr√≥geno? (Machine Learning)
\end{itemize}

\begin{agrobox}{Caso de Estudio: Finca ``La Esperanza''}
    El agr√≥nomo principal ha notado que el gasto en fertilizantes ha subido un 20\%, pero la producci√≥n est√° estancada.

    \textbf{Tu Misi√≥n:} Crear un modelo matem√°tico que nos diga exactamente \textit{cu√°l es la dosis √≥ptima} de Nitr√≥geno para maximizar el retorno de inversi√≥n, bas√°ndote en 5 a√±os de datos hist√≥ricos.
\end{agrobox}

% -----------------------------------------------------------------------------
\section{Fundamentos Matem√°ticos}
% -----------------------------------------------------------------------------

\subsection{Regresi√≥n Lineal Simple}
Imagina que queremos trazar una l√≠nea que represente la tendencia de nuestros datos. Matem√°ticamente, esta l√≠nea es nuestra \textbf{Hip√≥tesis} $h_\theta(x)$.

\vspace{0.5cm}

% Diagrama Explicativo de la Ecuaci√≥n
\begin{center}
    \begin{tikzpicture}
        \node[scale=1.5] (eq) at (0,0) {$h_\theta(x) = \textcolor{blue}{\theta_0} + \textcolor{red}{\theta_1} x$};

        \node[align=center, text=blue] (b) at (-3,-1.5) {\textbf{Intercepto (Sesgo)}\\Rendimiento base\\sin fertilizante};
        \draw[->, blue, thick] (b) -- (-0.8,-0.3);

        \node[align=center, text=red] (m) at (3,-1.5) {\textbf{Pendiente (Peso)}\\Kilos extra de caf√©\\por cada 1 ppm de N};
        \draw[->, red, thick] (m) -- (0.8,-0.3);
    \end{tikzpicture}
\end{center}

\subsection{Interpretaci√≥n Geom√©trica del Error}
No existe una l√≠nea perfecta que pase por todos los puntos. Siempre habr√° un error. Nuestro objetivo es que ese error sea el \textbf{m√≠nimo posible}.

\begin{center}
\begin{tikzpicture}[scale=1]
    \begin{axis}[
        axis lines=left,
        xlabel={Nitr√≥geno (ppm)},
        ylabel={Rendimiento (Ton/Ha)},
        ymin=0, ymax=5,
        xmin=0, xmax=10,
        width=10cm, height=6cm,
        grid=major,
        grid style={dashed, gray!30}
    ]
        % Datos Reales
        \addplot[only marks, mark=*, color=black] coordinates {
            (1, 1.2) (2, 1.8) (3, 2.5) (4, 3.0) (5, 3.2) (6, 4.1) (8, 4.5)
        };

        % L√≠nea de Regresi√≥n
        \addplot[domain=0:9, color=secondary, ultra thick] {0.5 + 0.5*x}
            node[pos=0.9, pin=135:{$h_\theta(x)$}] {};

        % Visualizaci√≥n del Error
        \draw[danger, thick, dashed] (axis cs:4,3.0) -- (axis cs:4,2.5);
        \node[danger, right] at (axis cs:4,2.75) {\small Error $(y - \hat{y})$};

        \draw[danger, thick, dashed] (axis cs:6,4.1) -- (axis cs:6,3.5);

    \end{axis}
\end{tikzpicture}
\end{center}

La l√≠nea verde es nuestro modelo. Las l√≠neas rojas punteadas son los \textbf{residuos}. El algoritmo intenta minimizar la suma de estas l√≠neas al cuadrado.

% -----------------------------------------------------------------------------
\section{El Algoritmo: Descenso del Gradiente}
% -----------------------------------------------------------------------------

\textquestiondown C√≥mo encuentra la computadora esa l√≠nea perfecta? Usa un algoritmo de optimizaci√≥n llamado \textit{Gradient Descent}.

\begin{mathconcept}{Analog√≠a del Monta√±ista}
    Imagina que est√°s en la cima de una monta√±a (donde la altura representa el \textbf{Error} del modelo) y hay niebla densa.

    \begin{enumerate}
        \item Sientes con el pie hacia d√≥nde est√° la pendiente m√°s inclinada hacia abajo.
        \item Das un paso en esa direcci√≥n (el tama√±o del paso es el \textbf{Learning Rate} $\alpha$).
        \item Repites hasta llegar al valle (Error M√≠nimo).
    \end{enumerate}
\end{mathconcept}

% -----------------------------------------------------------------------------
\section{Fundamentos de Optimizaci√≥n Convexa}
% -----------------------------------------------------------------------------

El coraz√≥n del Machine Learning no es la magia, es el c√°lculo multivariable. Para entender c√≥mo ``aprende'' la m√°quina, debemos formalizar dos componentes: la superficie de error y el mecanismo de descenso.

\subsection{La Funci√≥n de Costo (Loss Function)}

Sea un conjunto de entrenamiento con $m$ muestras, donde $x^{(i)}$ es la entrada y $y^{(i)}$ es la salida esperada. Nuestra hip√≥tesis es lineal: $h_\theta(x) = \theta_0 + \theta_1 x$.

Definimos la Funci√≥n de Costo $J(\theta)$ utilizando el Error Cuadr√°tico Medio (MSE), pero introducimos una modificaci√≥n escalar convencional:

\begin{equation}
    J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2
\end{equation}

\begin{mathconcept}{\textquestiondown Por qu√© $\frac{1}{2}$?}
    El factor $\frac{1}{2}$ es una conveniencia matem√°tica. Cuando derivemos esta funci√≥n (que tiene un exponente cuadr√°tico $^2$), el exponente bajar√° y se cancelar√° con el $\frac{1}{2}$, simplificando la ecuaci√≥n final del gradiente:
    $$ \frac{d}{dx} \left( \frac{1}{2} x^2 \right) = \frac{1}{2} \cdot 2x = x $$
\end{mathconcept}

Esta funci√≥n $J(\theta)$ es \textit{convexa}. Esto significa que tiene forma de ``taz√≥n'' (paraboloide), garantizando que cualquier m√≠nimo local es tambi√©n el \textit{m√≠nimo global}. No hay riesgo de quedar atrapado en valles falsos.

\subsection{El Gradiente Descendente (Formalizaci√≥n)}

El objetivo es minimizar $J(\theta)$. El gradiente ($\nabla J$) es un vector que contiene las derivadas parciales de la funci√≥n de costo respecto a cada par√°metro. Geom√©tricamente, el gradiente apunta hacia la direcci√≥n de mayor ascenso.

Por lo tanto, para minimizar el error, debemos movernos en la direcci√≥n \textit{opuesta} al gradiente:

\begin{equation}
    \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)
\end{equation}

Donde:
\begin{itemize}
    \item $\alpha$ (Alpha): Tasa de Aprendizaje. Controla la magnitud del paso.
    \item $\frac{\partial}{\partial \theta_j} J$: La derivada parcial (la pendiente tangente en ese punto).
\end{itemize}

\subsection{Derivaci√≥n Anal√≠tica de la Actualizaci√≥n}
\textquestiondown C√≥mo calculamos esa derivada parcial $\frac{\partial}{\partial \theta_j}$? Usamos la \textit{Regla de la Cadena}.
\textit{Demostraci√≥n para $\theta_1$ (la pendiente):}
\begin{enumerate}
\item Definimos el error de una muestra como $E = (h_\theta(x) - y)^2$.
\item Queremos derivar respecto a $\theta_1$:
   $$ \frac{\partial E}{\partial \theta_1} = 2(h_\theta(x) - y) \cdot \frac{\partial}{\partial \theta_1} (h_\theta(x) - y) $$
\item Como $h_\theta(x) = \theta_0 + \theta_1 x$, la derivada interna respecto a $\theta_1$ es simplemente $x$.
   $$ \frac{\partial E}{\partial \theta_1} = 2(h_\theta(x) - y) \cdot x $$
\item Sustituyendo en la sumatoria completa y cancelando el $\frac{1}{2}$:
\end{enumerate}
\begin{equation}
    \frac{\partial J}{\partial \theta_1} = \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right) \cdot x^{(i)}
\end{equation}

Esta es la f√≥rmula exacta que usa el algoritmo para calcular el siguiente paso.

\subsection{Ejemplo de Traza Visual: Una Iteraci√≥n a Mano}

Realicemos una iteraci√≥n manual paso a paso, acompa√±ando cada c√°lculo matem√°tico con su representaci√≥n gr√°fica para entender la geometr√≠a del aprendizaje.

\textbf{Escenario:}
\begin{itemize}
    \item \textbf{Dato Real:} $x=2$, $y=4$ (Punto objetivo).
    \item \textbf{Estado Inicial:} $\theta_1 = 0.5$ (L√≠nea muy acostada).
    \item \textbf{Hyperpar√°metros:} Learning Rate $\alpha = 0.1$.
\end{itemize}

% --- PASO 1: PREDICCI√ìN ---
\begin{agrobox}{Paso 1: Predicci√≥n y C√°lculo del Error}
    La m√°quina predice usando su modelo actual $h_\theta(x) = 0.5 x$.

    $$ \text{Predicci√≥n } (\hat{y}) = 0.5 \times 2 = 1.0 $$
    $$ \text{Error } (y - \hat{y}) = 4.0 - 1.0 = 3.0 $$

    Calculamos el Costo (MSE) para este punto:
    $$ J(\theta) = \frac{1}{2} (1.0 - 4.0)^2 = 4.5 $$

    \begin{center}
    \begin{tikzpicture}[scale=0.8]
        \begin{axis}[
            axis lines=left, xlabel={$x$ (Insumo)}, ylabel={$y$ (Salida)},
            xmin=0, xmax=3, ymin=0, ymax=5,
            grid=major, width=8cm, height=6cm
        ]
            % Punto Real
            \addplot[only marks, mark=*, color=blue, mark size=3pt] coordinates {(2,4)} node[above] {Realidad (2,4)};
            % Predicci√≥n
            \addplot[only marks, mark=x, color=red, mark size=4pt] coordinates {(2,1)} node[below right] {Predicci√≥n (2,1)};
            % L√≠nea Actual
            \addplot[domain=0:3, color=red, thick] {0.5*x} node[pos=0.8, sloped, above] {$\theta=0.5$};
            % Visualizaci√≥n del Error
            \draw[<->, dashed, thick, blue] (axis cs:2,1) -- (axis cs:2,4) node[midway, right] {Error = 3.0};
        \end{axis}
    \end{tikzpicture}
    \end{center}
    \small \textit{Interpretaci√≥n: La l√≠nea roja est√° muy lejos del punto azul. El error es la l√≠nea punteada.}
\end{agrobox}

\newpage

% --- PASO 2: GRADIENTE ---
\begin{agrobox}{Paso 2: C√°lculo del Gradiente (La Br√∫jula)}
    Aqu√≠ cambiamos de perspectiva. No miramos los datos ($x$ vs $y$), sino el \textbf{Error vs Par√°metro} ($\theta$ vs $J$).

    $$ \frac{\partial J}{\partial \theta_1} = (\text{Pred} - \text{Real}) \cdot x $$
    $$ \text{Gradiente} = (1.0 - 4.0) \cdot 2 = -6.0 $$

    El gradiente es la pendiente de la curva de error. Al ser negativo (-6.0), nos dice que la pendiente ``baja'' hacia la derecha.

    \begin{center}
    \begin{tikzpicture}[scale=0.8]
        \begin{axis}[
            axis lines=left, xlabel={$\theta_1$ (Valor del Par√°metro)}, ylabel={$J(\theta)$ (Costo/Error)},
            xmin=0, xmax=3, ymin=0, ymax=10,
            grid=major, width=8cm, height=6cm
        ]
            % Funci√≥n de Costo J(theta) = 1/2 * (theta*2 - 4)^2 = 2(theta-2)^2
            \addplot[domain=0:3, color=black, thick, smooth] {2*(x-2)^2};

            % Punto actual
            \addplot[only marks, mark=*, color=red] coordinates {(0.5, 4.5)};

            % Tangente (Gradiente) en x=0.5. Pendiente = 4(0.5-2) = -6
            % y - 4.5 = -6(x - 0.5) => y = -6x + 3 + 4.5 => y = -6x + 7.5
            \addplot[domain=0.2:0.8, color=blue, thick, ->] {7.5 - 6*x};

            \node[right, color=blue] at (axis cs:0.5, 2.5) {Gradiente = -6.0};
            \node[align=center] at (axis cs:2, 1) {\textbf{M√≠nimo Global}\\(Meta)};
        \end{axis}
    \end{tikzpicture}
    \end{center}
    \small \textit{Interpretaci√≥n: Estamos en la bola roja. La pendiente es muy inclinada hacia abajo a la derecha. Debemos movernos a la derecha.}
\end{agrobox}

% --- PASO 3: ACTUALIZACI√ìN ---
\begin{agrobox}{Paso 3: Actualizaci√≥n (El Aprendizaje)}
    La m√°quina ajusta su visi√≥n del mundo bas√°ndose en el gradiente.

    $$ \theta_{nuevo} = \theta_{viejo} - \alpha \cdot (\text{Gradiente}) $$
    $$ \theta_{nuevo} = 0.5 - 0.1 \cdot (-6.0) $$
    $$ \theta_{nuevo} = 0.5 + 0.6 = \textbf{1.1} $$

    Volvemos al mundo real ($x$ vs $y$) para ver el cambio:

    \begin{center}
    \begin{tikzpicture}[scale=0.8]
        \begin{axis}[
            axis lines=left, xlabel={$x$}, ylabel={$y$},
            xmin=0, xmax=3, ymin=0, ymax=5,
            grid=major, width=8cm, height=6cm,
            legend pos=north west
        ]
            % Punto Real
            \addplot[only marks, mark=*, color=blue] coordinates {(2,4)};

            % L√≠nea Vieja
            \addplot[domain=0:3, color=gray, dashed, thick] {0.5*x};
            \addlegendentry{Antes ($\theta=0.5$)}

            % L√≠nea Nueva
            \addplot[domain=0:3, color=green!60!black, ultra thick] {1.1*x};
            \addlegendentry{Ahora ($\theta=1.1$)}

            % Flecha de mejora
            \draw[->, thick, orange] (axis cs:2.2, 1.1) -- (axis cs:2.2, 2.2) node[midway, right] {¬°Mejora!};
        \end{axis}
    \end{tikzpicture}
    \end{center}
    \small \textit{Interpretaci√≥n: La l√≠nea verde (nuevo modelo) ha ``saltado'' hacia arriba, acerc√°ndose a la realidad. Si repetimos esto 5 veces m√°s, llegar√° al punto azul.}
\end{agrobox}%

% -----------------------------------------------------------------------------
\newpage
\section{Simulaci√≥n Computacional: El Algoritmo al Desnudo}
% -----------------------------------------------------------------------------

Hasta ahora hemos hecho una sola iteraci√≥n a mano. Pero el poder de la computadora radica en realizar millones de estas operaciones por segundo.

A continuaci√≥n, implementamos el \textit{Descenso del Gradiente Estoc√°stico (SGD)} desde cero en Python. Este script replica exactamente la matem√°tica que acabamos de derivar, mostrando c√≥mo el par√°metro $\theta_1$ evoluciona iteraci√≥n tras iteraci√≥n hasta encontrar el valor √≥ptimo.

\subsection{Script de Laboratorio: \texttt{gradient\_descent\_lab.py}}

\begin{lstlisting}[language=Python, caption={Implementaci√≥n manual del algoritmo de optimizaci√≥n}]
import time

print("--- üî¨ INICIO DE SIMULACI√ìN DE APRENDIZAJE ---")

# 1. CONFIGURACI√ìN DEL ENTORNO (Datos y Par√°metros)
x_real = 2.0   # Insumo (Nitr√≥geno)
y_real = 4.0   # Salida esperada (Rendimiento)

theta = 0.5    # Conocimiento inicial (Aleatorio/Err√≥neo)
alpha = 0.1    # Tasa de Aprendizaje (Learning Rate)
iteraciones = 10

print(f"Meta: Aprender que y = 2x (Target Theta = 2.0)")
print(f"Estado Inicial: Theta = {theta}, Alpha = {alpha}\n")
print(f"{'ITER':<5} | {'PRED':<8} | {'ERROR':<8} | {'GRADIENTE':<10} | {'NUEVO THETA':<12}")
print("-" * 60)

# 2. BUCLE DE APRENDIZAJE (El "Cerebro")
for i in range(1, iteraciones + 1):

    # A. FORWARD PASS (Predicci√≥n)
    prediccion = theta * x_real

    # B. C√ÅLCULO DEL ERROR
    error = prediccion - y_real
    costo = 0.5 * (error ** 2)

    # C. BACKPROPAGATION (C√°lculo del Gradiente)
    # Derivada: (h(x) - y) * x
    gradiente = error * x_real

    # D. ACTUALIZACI√ìN DE PAR√ÅMETROS (Optimizaci√≥n)
    theta_anterior = theta
    theta = theta - (alpha * gradiente)

    # Visualizaci√≥n de datos
    print(f"{i:<5} | {prediccion:<8.4f} | {error:<8.4f} | {gradiente:<10.4f} | {theta:<12.4f}")

    # Pausa dram√°tica para efecto visual en consola
    # time.sleep(0.5)

print("-" * 60)
print(f"‚úÖ FINALIZADO.")
print(f"Valor Real Ideal: 2.0000")
print(f"Valor Aprendido:  {theta:.4f}")
print("--- CONVERGENCIA ALCANZADA ---")
\end{lstlisting}

\subsection{An√°lisis de la Ejecuci√≥n}

Al ejecutar el c√≥digo anterior, observamos c√≥mo la m√°quina corrige su error agresivamente al principio y luego hace ajustes finos.

\begin{tcolorbox}[
    colback=black!85!white,
    colframe=gray,
    title={\texttt{>\_ Salida de Terminal}},
    fonttitle=\bfseries\ttfamily,
    colupper=green!70!white, % Texto verde estilo hacker
    fontupper=\ttfamily\footnotesize
]
--- üî¨ INICIO DE SIMULACI√ìN DE APRENDIZAJE ---\\
Meta: Aprender que y = 2x (Target Theta = 2.0)\\
Estado Inicial: Theta = 0.5, Alpha = 0.1

ITER  | PRED     | ERROR    | GRADIENTE  | NUEVO THETA \\
------------------------------------------------------------\\
1     | 1.0000   | -3.0000  | -6.0000    | 1.1000      \\
2     | 2.2000   | -1.8000  | -3.6000    | 1.4600      \\
3     | 2.9200   | -1.0800  | -2.1600    | 1.6760      \\
4     | 3.3520   | -0.6480  | -1.2960    | 1.8056      \\
5     | 3.6112   | -0.3888  | -0.7776    | 1.8834      \\
6     | 3.7667   | -0.2333  | -0.4666    | 1.9300      \\
7     | 3.8600   | -0.1400  | -0.2799    | 1.9580      \\
8     | 3.9160   | -0.0840  | -0.1680    | 1.9748      \\
9     | 3.9496   | -0.0504  | -0.1008    | 1.9849      \\
10    | 3.9698   | -0.0302  | -0.0605    | 1.9909      \\
------------------------------------------------------------\\
‚úÖ FINALIZADO.\\
Valor Real Ideal: 2.0000\\
Valor Aprendido:  1.9909
\end{tcolorbox}

\begin{agrobox}{Observaciones Clave}
    \begin{enumerate}
        \item \textbf{La velocidad de aprendizaje disminuye:} Nota como en la iteraci√≥n 1, $\theta$ salt√≥ de $0.5 \rightarrow 1.1$ (+0.6). Sin embargo, en la iteraci√≥n 10, solo salt√≥ de $1.98 \rightarrow 1.99$ (+0.01).
        \item \textbf{\textquestiondown Por qu√©?} A medida que el error se acerca a cero, el gradiente tambi√©n se acerca a cero. Matem√°ticamente: $Gradiente \to 0 \implies Pasos \to 0$.
        \item \textbf{Convergencia:} El modelo se ``estabiliza'' solo. No necesitamos decirle cu√°ndo parar, simplemente deja de moverse cuando ya no hay error que corregir.
    \end{enumerate}
\end{agrobox}

% -----------------------------------------------------------------------------
\newpage
\section{Implementaci√≥n Profesional: Scikit-Learn}
% -----------------------------------------------------------------------------

Ahora que comprendemos la mec√°nica interna del aprendizaje (el motor), no necesitamos construir el auto desde cero cada vez que queremos conducir. En el entorno profesional, utilizamos librer√≠as optimizadas en C/C++ que realizan estos c√°lculos de forma instant√°nea y robusta.

La librer√≠a est√°ndar en la industria es \texttt{scikit-learn}. A continuaci√≥n, ver√°s c√≥mo todo el c√≥digo manual anterior se reduce a unas pocas l√≠neas potentes.

\subsection{Script de Producci√≥n: \texttt{modelo\_pro.py}}

\begin{lstlisting}[language=Python, caption={Regresi√≥n Lineal usando Scikit-Learn}]
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

print("--- üè≠ INICIANDO PIPELINE INDUSTRIAL ---")

# 1. DATOS (Simulaci√≥n de un dataset real)
# X: Nitr√≥geno aplicado (Matriz 2D requerida por Sklearn)
# y: Rendimiento observado (Vector)
X_train = np.array([[1.0], [2.0], [3.0], [4.0], [5.0]])
y_train = np.array([[2.1], [3.9], [6.2], [8.1], [9.9]])

# 2. INSTANCIACI√ìN
# Aqu√≠ cargamos el algoritmo OLS (Ordinary Least Squares),
# una versi√≥n anal√≠tica avanzada del Descenso del Gradiente.
modelo = LinearRegression()

# 3. ENTRENAMIENTO (FIT)
# En esta sola l√≠nea ocurre toda la magia matem√°tica:
# Calcula el error, deriva y ajusta los par√°metros.
modelo.fit(X_train, y_train)

# 4. EXTRACCI√ìN DE CONOCIMIENTO
theta_0 = modelo.intercept_[0] # Sesgo (b)
theta_1 = modelo.coef_[0][0]   # Pendiente (m)

print(f"‚úÖ Modelo Entrenado.")
print(f" -> F√≥rmula: y = {theta_0:.2f} + {theta_1:.2f}x")

# 5. EVALUACI√ìN Y PREDICCI√ìN
y_pred = modelo.predict(X_train)
r2 = r2_score(y_train, y_pred)

print(f" -> Precisi√≥n (R2 Score): {r2:.4f} (99.8% de ajuste)")

# Predicci√≥n para un nuevo cliente
nuevo_suelo = [[3.5]]
prediccion = modelo.predict(nuevo_suelo)
print(f"üîÆ Predicci√≥n para 3.5 ppm N: {prediccion[0][0]:.2f} Ton/Ha")
\end{lstlisting}

\subsection{Interpretaci√≥n de la Salida}

\begin{tcolorbox}[
    colback=black!85!white,
    colframe=blue!80!black,
    title={\texttt{>\_ Consola de Producci√≥n}},
    fonttitle=\bfseries\ttfamily,
    colupper=white,
    fontupper=\ttfamily\footnotesize
]
--- üè≠ INICIANDO PIPELINE INDUSTRIAL ---\\
‚úÖ Modelo Entrenado.\\
 -> F√≥rmula: y = 0.10 + 1.96x\\
 -> Precisi√≥n (R2 Score): 0.9984 (99.8\% de ajuste)\\
üîÆ Predicci√≥n para 3.5 ppm N: 6.96 Ton/Ha
\end{tcolorbox}

\begin{agrobox}{An√°lisis Comparativo}
    \begin{itemize}
        \item \textbf{Eficiencia:} Mientras nuestro script manual tard√≥ 10 iteraciones para acercarse a $\theta=1.99$, Scikit-Learn calcul√≥ el √≥ptimo $\theta=1.96$ en microsegundos usando √°lgebra lineal matricial.
        \item \textbf{El intercepto:} Scikit-Learn detect√≥ autom√°ticamente que los datos ten√≠an un peque√±o ``ruido'' o base inicial ($0.10$), algo que en nuestro modelo manual simplificado hab√≠amos ignorado.
        \item \textbf{Score $R^2$:} Este n√∫mero es vital para el negocio. Un 0.9984 significa que el modelo explica el 99.8\% de la variabilidad del rendimiento. Es un modelo extremadamente confiable.
    \end{itemize}
\end{agrobox}

\subsection{Visualizaci√≥n del Ajuste Final}

A diferencia de las iteraciones anteriores, aqu√≠ vemos el resultado final: una l√≠nea que minimiza la distancia a todos los puntos simult√°neamente.

\begin{center}
\begin{tikzpicture}
    \begin{axis}[
        title={Regresi√≥n Lineal Final (Scikit-Learn)},
        xlabel={Nitr√≥geno (kg/ha)},
        ylabel={Rendimiento (Ton)},
        grid=major,
        width=10cm, height=7cm,
        legend pos=north west,
        % --- FIX: Aumentamos un poco los m√°rgenes si es necesario ---
        % clip=false % Descomentar si la etiqueta sigue cort√°ndose
    ]
        % Puntos de entrenamiento (Simulados del c√≥digo)
        \addplot[only marks, mark=*, color=blue, mark size=2.5pt] coordinates {
            (1.0, 2.1) (2.0, 3.9) (3.0, 6.2) (4.0, 8.1) (5.0, 9.9)
        };
        \addlegendentry{Datos Reales}

        % L√≠nea de regresi√≥n: y = 0.10 + 1.96x
        \addplot[domain=0:6, color=red, ultra thick] {0.10 + 1.96*x};
        \addlegendentry{Modelo $h_\theta(x)$}

        % Proyecci√≥n de la predicci√≥n (3.5, 6.96)
        \addplot[mark=square*, color=green!60!black, mark size=3pt] coordinates {(3.5, 6.96)};

        % --- FIX: Cambiamos pin=320 por pin=135 para apuntar arriba-izquierda ---
        \node[pin=135:{Predicci√≥n (3.5, 6.96)}] at (axis cs:3.5, 6.96) {};

        % L√≠neas de ayuda para la predicci√≥n
        \draw[dashed, help lines] (axis cs:3.5,0) -- (axis cs:3.5,6.96) -- (axis cs:0,6.96);
    \end{axis}
\end{tikzpicture}
\end{center}
% -----------------------------------------------------------------------------
\newpage
\section{Proyecto de Cierre: El Consultor de Datos}
% -----------------------------------------------------------------------------

Para finalizar este m√≥dulo, uniremos todas las piezas (Matem√°ticas, Python y Negocio) en un script profesional.

\textbf{El Reto:} Tu cliente necesita un reporte autom√°tico que no solo calcule la f√≥rmula, sino que tambi√©n \textbf{valide} si el modelo es lo suficientemente confiable para usarlo en campo y genere una gr√°fica para la junta directiva.

\subsection{Script Maestro: \texttt{pipeline\_final.py}}

\begin{lstlisting}[language=Python, caption={Pipeline completo con Evaluaci√≥n y Visualizaci√≥n}]
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error

# --- 1. INGESTA DE DATOS ---
print("üì• Cargando datos de campo...")
df = pd.read_csv('suelos_cosecha.csv')
X = df[['nitrogeno_ppm']]
y = df['rendimiento_ton_ha']

# --- 2. VALIDACI√ìN CRUZADA ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- 3. MODELADO ---
modelo = LinearRegression()
modelo.fit(X_train, y_train)

# --- 4. EVALUACI√ìN RIGUROSA ---
y_pred = modelo.predict(X_test)
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)

print(f"\n--- üìä REPORTE DE CALIDAD ---")
print(f"R¬≤ (Explicabilidad): {r2:.2%}")
print(f"Error Promedio (MSE): {mse:.2f}")

# --- 5. TOMA DE DECISIONES AUTOM√ÅTICA ---
UMBRAL_CALIDAD = 0.80 # Exigimos al menos 80% de precisi√≥n

if r2 >= UMBRAL_CALIDAD:
    print("‚úÖ ESTADO: MODELO APROBADO PARA USO.")
    print(f"   F√≥rmula: Rendimiento = {modelo.intercept_:.2f} + {modelo.coef_[0]:.3f} * Nitr√≥geno")
else:
    print("‚ö†Ô∏è ESTADO: MODELO RECHAZADO.")
    print("   Acci√≥n: Recolectar m√°s datos o buscar otras variables (pH, Lluvia).")

# --- 6. VISUALIZACI√ìN DEL REPORTE ---
plt.figure(figsize=(10, 6))
plt.scatter(X_test, y_test, color='black', label='Datos Reales (Test)')
plt.plot(X_test, y_pred, color='green', linewidth=3, label='Modelo Lineal')
plt.title(f'Proyecci√≥n de Rendimiento (R¬≤: {r2:.2f})')
plt.xlabel('Nitr√≥geno (ppm)')
plt.ylabel('Rendimiento (Ton/Ha)')
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.savefig('reporte_gerencial.png')
print("\nüñºÔ∏è Gr√°fica guardada como 'reporte_gerencial.png'")
\end{lstlisting}

\begin{warningbox}{Peligro: El Riesgo de Extrapolaci√≥n}
    El modelo funciona bien dentro del rango de datos que conoce (ej: 0 a 150 ppm).
    \textbf{Nunca uses el modelo para predecir valores extremos.} Si preguntas qu√© pasa con \textbf{5000 ppm} de nitr√≥geno, el modelo matem√°tico dir√° que tendr√°s una cosecha gigante, pero la realidad biol√≥gica es que \textbf{matar√°s la planta} por toxicidad.
\end{warningbox}

% -----------------------------------------------------------------------------
\section{Conclusiones y Siguientes Pasos}
% -----------------------------------------------------------------------------

Al completar esta semana, has adquirido una superpotencia: la capacidad de ver el futuro a trav√©s de los datos.

\begin{itemize}
    \item \textbf{Matem√°ticamente:} Entiendes que ``aprender'' es minimizar una funci√≥n de costo usando derivadas.
    \item \textbf{Computacionalmente:} Sabes implementar el Descenso del Gradiente y usar Scikit-Learn.
    \item \textbf{Estrat√©gicamente:} Sabes que un modelo sin interpretaci√≥n es in√∫til.
\end{itemize}

\begin{agrobox}{Cu√°ndo NO usar regresi√≥n lineal}
\begin{itemize}
    \item Relaci√≥n no lineal (ej.: rendimiento vs. pH tiene un √≥ptimo en 6.5).
    \item Interacciones entre variables (ej.: nitr√≥geno solo funciona si hay suficiente agua).
    \item Datos con muchos outliers (sensores fallidos no filtrados).
\end{itemize}
En estos casos, necesitaremos modelos m√°s flexibles (√°rboles, redes neuronales).
\end{agrobox}

\subsection*{\textquestiondown Qu√© sigue?}
El mundo real rara vez depende de una sola variable. En la pr√≥xima semana, abordaremos la \textbf{Regresi√≥n Lineal M√∫ltiple}, donde aprenderemos a predecir la cosecha combinando Nitr√≥geno, pH, Lluvias y Radiaci√≥n Solar simult√°neamente.

\vspace{1cm}

\begin{agrobox}{Tu Entregable}
    Sube a la plataforma el archivo \texttt{pipeline\_final.py} y la imagen \texttt{reporte\_gerencial.png} generada con tus propios datos simulados.
\end{agrobox}

\end{document}
